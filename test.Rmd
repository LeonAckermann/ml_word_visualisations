---
title: "R Notebook"
output: html_notebook
---

## using LDA model of textmineR
- mallet will be supported later
- data is private therefore not included in repository


```{r}
data_dir <- "response_format_cleaned_ds1.csv"
text <- read.csv(paste0("./data/", data_dir))
#text <- read.csv('response_format_cleaned_ds1.csv') # load text
id_col <- "participant_id"
data_col <- "wor_text"
group_col <- "miniGAD_diagnose"
text_cols= text[c(id_col,data_col,group_col)] # select columns
text_cols <- text_cols[complete.cases(text_cols), ] # remove rows without values
#text_cols = text_cols[sample(1:nrow(text_cols)), ]
#split_index <- round(nrow(text_cols) * split) # 
#train <- df[1:split_index, ]
#test <- df[split_index:nrow(text_cols), ]
# create a document term matrix
  
dtm <- CreateDtm(doc_vec = text_cols[[data_col]], # character vector of documents
                   doc_names = text_cols[[id_col]], # document names
                   ngram_window = c(1, 3), # minimum and maximum n-gram length
                   stopword_vec = stopwords::stopwords("en", source = "snowball"),
                   lower = TRUE, # lowercase - this is the default value
                   remove_punctuation = TRUE, # punctuation - this is the default
                   remove_numbers = TRUE, # numbers - this is the default
                   verbose = FALSE, # Turn off status bar for this demo
                   cpus = 4) # default is all available cpus on the system
dtm <- dtm[,colSums(dtm) > 2] # remove words with occurences < 2
view(dtm)
dtm$
```

```{r}
source("./lda/main.R")

data <- readRDS("./data/cantril_s1_no_id.rds")
data$best_life_no_ladder <- paste(data$BestLife.SQ001., data$BestLife.SQ002., data$BestLife.SQ003., data$BestLife.SQ004., data$BestLife.SQ005.)
data$best_life_ladder <- paste(data$LOLupp.SQ001., data$LOLupp.SQ002., data$LOLupp.SQ003., data$LOLupp.SQ004., data$LOLupp.SQ005.)
data$best_life <- paste(data$best_life_no_ladder, data$best_life_ladder)
```

```{r}
filtered_df <- subset(data, Sex != "CONSENT REVOKED")
filtered_df <- subset(filtered_df, age != "CONSENT REVOKED")
# Assuming your data frame is called "df"
filtered_df <- filtered_df %>% mutate(Sex = ifelse(Sex == "Male", 0, ifelse(Sex == "Female", 1, Sex)))
filtered_df <- filtered_df %>% mutate(randnumber = ifelse(randnumber == 1, 0, ifelse(randnumber == 2, 1, randnumber)))
filtered_df$randnumber <- as.numeric(filtered_df$randnumber)
filtered_df$Sex <- as.numeric(filtered_df$Sex)
filtered_df[is.na(filtered_df)] <- ''
filtered_df$current_score <- paste(filtered_df$LolwithoutLadder, filtered_df$LOL, sep="")
filtered_df$current_score <- as.numeric(filtered_df$current_score)
view(filtered_df)
saveRDS(filtered_df, "./data/cantril_1_cleaned_filtered.rds")
```

```{r}
library(tm)
text <- readRDS("./data/cantril_1_cleaned_filtered.rds")
text <- paste(text$best_life, collapse = " ")
words <- unlist(strsplit(text, " "))

# Define a list of stopwords
stopwords_list <- stopwords("en")

# Remove stopwords
filtered_words <- words[!tolower(words) %in% stopwords_list]
# Count word frequencies
word_freq <- table(words)
# Sort word frequencies in descending order
sorted_word_freq <- sort(word_freq, decreasing = TRUE)

# Get the top 10 most frequent words
top_10_words <- names(sorted_word_freq[1:10])

# Print the top 10 words
print(top_10_words)
```

```{r}
source("./lda/main.R")
dtms <- get_dtm(data_dir = "./data/cantril_1_cleaned_filtered.rds",
                id_col = "id",
                data_col = "best_life",
                group_var = "randnumber",
                ngram_window = c(1,2),
                stopwords = NULL,
                removalword = "",
                split=1,
                seed=1234)
```

```{r}
# Sum the frequencies of each term in the DTM

dtm <- dtms$train_dtm
term_frequencies <- colSums(as.matrix(dtm))
df <- data.frame(as.matrix(dtm))

# Create a data frame with term and frequency
term_frequency_df <- data.frame(Term = colnames(dtm), Frequency = term_frequencies)

# Sort the data frame in descending order of frequency
term_frequency_df <- term_frequency_df[order(-term_frequency_df$Frequency), ]

# Print the terms with the highest frequencies (e.g., top 10 terms)
top_terms <- head(term_frequency_df, n = 10)
print(top_terms)
#df$B <- NULL
df <- subset(df, select = -family)
DocumentTermMatrix(df)
```
```{r}
# Calculate word frequencies from the DTM
word_frequencies <- colSums(dtms$train_dtm)

# Create a data frame with words and their frequencies
word_freq_df <- data.frame(Word = names(word_frequencies), Frequency = word_frequencies)

# Sort the data frame by frequency in descending order
word_freq_df <- word_freq_df[order(-word_freq_df$Frequency), ]

# Display the top N most frequent words (change N to the number of words you want to display)
N <- 10  # Change this to the desired number of top words
top_words <- head(word_freq_df, N)

# Print the top N words and their frequencies
print(top_words)

```
```{r}
source("./lda/main.R")
model <- get_lda_model(model_type="mallet",
                       dtm=dtms$train_dtm,
                       num_topics=30,
                       num_top_words=10,
                       num_iterations = 2000,
                       seed=1234)
```

```{r}
model$summary
```

```{r}
source("./lda/main.R")

preds <- get_lda_preds(model = model,
                       num_iterations=500,
                       data = dtms$train_data,
                       dtm = dtms$train_dtm,
                       group_var = c("randnumber",
                                     "Sex",
                                     "age",
                                     "current_score"),
                       seed=1234)

```
```{r}
model$summary
```

```{r}
plot(preds$t_48, preds$randnumber)
```


```{r}
source("./lda/main.R")

test <- get_lda_test(model=model,
                     preds=preds,
                     group_var = "randnumber",
                     control_vars = c("randnumber"),
                     test_method = "logistic_regression",
                     seed=1234)
view(test)

```

```{r}
view(test)
```

```{r}
preds_old <- readRDS("./pos_randnumber_mallet_200/seed_1/preds.rds")
view(preds_old)
```


```{r}
# Initialize an empty list to store the topic names
lda_topics <- character(150)

# Create the list of LDA topics
for (i in 1:150) {
  lda_topics[i] <- paste("t_", i, sep = "")
}
# Loop through each LDA topic and create a linear model
simple_models <- list()

for (topic in lda_topics) {
  formula <- as.formula(paste(topic, "~ miniGAD_scale"))
  simple_models[[paste0("t_",topic)]] <- lm(formula, data = preds)
}
```
```{r}
summary(simple_models$)
```

```{r}
significant_models <- c()  # To store the indices or names of significant models

for (i in 1:length(simple_models)) {
  temp <- simple_models[[i]]  # Assuming you have a list of linear models

  # Extract p-values for all coefficients in the model
  p_values <- summary(temp)$coefficients[, "Pr(>|t|)"]["miniGAD_scale"]

  # Check if any p-value is below 0.05
  if (any(p_values < 0.05)) {
    significant_models <- c(significant_models, i)
  }
}
view(significant_models)
```

```{r}
# Initialize an empty list to store the topic names
lda_topics <- character(150)

# Create the list of LDA topics
for (i in 1:150) {
  lda_topics[i] <- paste("t_", i, sep = "")
}


for (topic in lda_topics) {
  preds[[paste0("z_",topic)]] <- scale(preds[[topic]])
}

preds$z_age <- scale(preds$Age)
preds$z_gender <- scale(preds$Gender)
preds$z_miniGAD_dianose <- scale(preds$miniGAD_diagnose)
preds$z_miniGAD_scale <- scale(preds$miniGAD_scale)
preds$z_minidep_scale <- scale(preds$minidep_scale)
# Initialize an empty list to store the topic names
z_lda_topics <- character(150)

# Create the list of LDA topics
for (i in 1:150) {
  z_lda_topics[i] <- paste("z_t_", i, sep = "")
}

# Loop through each LDA topic and create a linear model
multi_models <- list()

for (topic in z_lda_topics) {
  formula <- as.formula(paste(topic, "~ z_miniGAD_scale + z_minidep_scale + z_age + z_gender"))
  multi_models[[paste0("t_",topic)]] <- lm(formula, data = preds)
}

multi_models

```

```{r}
summary(multi_models$t_z_t_1)$coefficients
```

```{r}
significant_topics <- c()  # To store the indices or 
significant_p_z_age <- c()
significant_p_z_gender <- c()
significant_p_z_miniGAD_scale <- c() # To store the indices or names of significant models
significant_p_z_minidep_scale <- c()
t_age <- c()
t_gender <- c()
t_GAD <- c()
t_dep <- c()
estimate_age <- c()
estimate_gender <- c()
estimate_GAD <- c()
estimate_dep <- c()

for (i in 1:length(multi_models)) {
  temp <- multi_models[[i]]  # Assuming you have a list of linear models

  # Extract p-values for all coefficients in the model
  p_values <- summary(temp)$coefficients[, 4]
  t_values <- summary(temp)$coefficients[, "t value"]
  estimate_values <- summary(temp)$coefficients[, "Estimate"]
  
  # Check if any p-value is below 0.05
  significant_topics <- c(significant_topics, paste0("t_",i))
  significant_p_z_age <- c(significant_p_z_age,p_values[["z_age"]])
  significant_p_z_gender <- c(significant_p_z_gender,p_values[["z_gender"]])
  significant_p_z_miniGAD_scale <- c(significant_p_z_miniGAD_scale,p_values[["z_miniGAD_scale"]])
  significant_p_z_minidep_scale <- c(significant_p_z_minidep_scale, p_values[["z_minidep_scale"]])
  t_age <- c(t_age, t_values[["z_age"]])
  t_gender <- c(t_gender, t_values[["z_gender"]])
  t_GAD <- c(t_GAD, t_values[["z_miniGAD_scale"]])
  t_dep <- c(t_dep, t_values[["z_minidep_scale"]])
  estimate_age <- c(estimate_age, estimate_values[["z_age"]])
  estimate_gender <- c(estimate_gender, estimate_values[["z_gender"]])
  estimate_GAD <- c(estimate_GAD, estimate_values[["z_miniGAD_scale"]])
  estimate_dep <- c(estimate_dep, estimate_values[["z_minidep_scale"]])
}

view(significant_topics)
p_GAD_adjusted <- stats::p.adjust(significant_p_z_miniGAD_scale, "bonferroni",4)
significant_multi_models <- data.frame(topic=significant_topics,
                                       estimate_GAD = estimate_GAD,
                                       t_GAD = t_GAD,
                                       p_GAD = significant_p_z_miniGAD_scale,
                                       p_GAD_adjusted = p_GAD_adjusted,
                                       estimate_dep = estimate_dep,
                                       t_dep = t_dep,
                                       z_dep = significant_p_z_minidep_scale,
                                       estimate_age = estimate_age,
                                       t_age = t_age,
                                       p_age = significant_p_z_age,
                                       estimate_gender = estimate_gender,
                                       t_gender = t_gender,
                                       p_gender = significant_p_z_gender)
view(significant_multi_models)
```

```{r}
view(model$summary[c("topic", "top_terms")])
```

```{r}
class(significant_multi_models$topic)
output <- dplyr::right_join(model$summary[c("topic", "top_terms")], 
                            significant_multi_models,
                            by=join_by(topic))
view(output)
```

```{r}
plot(preds[c("t_17", "Age", "Gender", "miniGAD_scale")])
```


```{r}
summary(models$t_t_1)
```
```{r}
test <- get_lda_test(model=model,
                     preds=preds,
                     group_var = "miniGAD_diagnose",
                     test_method = "t-test",
                     seed="1")

```
```{r}
source("./lda/main.R")
test <- get_topic_test(model_type = "mallet",
                       num_topics = 200,
                       num_top_words = 10,
                       data_dir = "cantril_1_cleaned.csv",
                       id_column = "id",
                       data_column = "pos",
                       diagnose_column = "randnumber",
                       save=TRUE,
                       load=FALSE,
                       seed=1)
view(test)
```


```{r}
data <- readRDS("./data/cantril_s1_no_id.rds")
data$best_life <- paste(data$BestLife.SQ001., data$BestLife.SQ002., data$BestLife.SQ003., data$BestLife.SQ004., data$BestLife.SQ005.)
data$worst_life <- paste(data$WorstLife.SQ001., data$WorstLife.SQ002., data$WorstLife.SQ003., data$WorstLife.SQ004., data$WorstLife.SQ005.)
data$ladder_down <- paste(data$LOLdown.SQ001., data$LOLdown.SQ002., data$LOLdown.SQ003., data$LOLdown.SQ004., data$LOLdown.SQ005.)
data$ladder_upp <- paste(data$LOLupp.SQ001., data$LOLupp.SQ002., data$LOLupp.SQ003., data$LOLupp.SQ004., data$LOLupp.SQ005.)
data$pos <- paste0(data$best_life, data$ladder_upp)
data$neg <- paste0(data$worst_life, data$ladder_down)
write.csv(data, "./data/cantril_1_cleaned.csv")
```

```{r}
data2 <- readRDS("./data/cantril_s2_no_id.rds")
view(data2)
```

```{r}

library(tidyverse)
#library(text)
data <- read_csv("./data/response_format_cleaned_ds1.csv")
data_frame <- data.frame(data$wor_text)#
#textrpp_initialize(condaenv = "text_package")
```

```{r}
# Transform the text data to BERT word embeddings
library(text)
library(reticulate)
use_condaenv("text_package", required = TRUE)
word_embeddings <- textEmbed(
  texts = data$dep_text,
  model = "bert-base-uncased",
  layers = -2,
  aggregation_from_tokens_to_texts = "mean",
  aggregation_from_tokens_to_word_types = "mean",
  keep_token_embeddings = FALSE)
```

```{r}
projection_results <- textProjection(
  words = data$dep_text,
  word_embeddings = word_embeddings$texts,
  word_types_embeddings = word_embeddings$word_types,
  x = data$miniGAD_diagnose,
  Npermutations = 50000
)
```
```{r}
projection_results$word_data
```

```{r}
# Supervised Dimension Projection Plot
# To avoid warnings -- and that words do not get plotted, first increase the max.overlaps for the entire session: 
options(ggrepel.max.overlaps = 1000)

# Supervised Dimension Projection Plot
plot_projection_2D <- textProjectionPlot(
  word_data = projection_results,
 min_freq_words_plot = 1,
 plot_n_word_extreme = 10,
 plot_n_word_frequency = 5,
 plot_n_words_middle = 5,
 y_axes = FALSE,
 p_alpha = 0.05,
 p_adjust_method = "fdr",
  title_top = "Harmony Words Responses (Supervised Dimension Projection)",
  x_axes_label = "Low vs. High Harmony in Life Scale Score",
  y_axes_label = "",
  bivariate_color_codes = c( "#FFFFFF","#FFFFFF" ,"#FFFFFF" , 
                             "#E07F6A","#EAEAEA" ,"#85DB8E" , 
                             "#FFFFFF","#FFFFFF" ,"#FFFFFF"
))
# View plot
plot_projection_2D$final_plot
```

```{r}
centrality_results <- textCentrality(words = data$dep_text, 
                                     word_embeddings = word_embeddings$texts, 
                                     word_types_embeddings = word_embeddings$word_types)
```